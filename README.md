# MiniMind-in-Depth

> 🌱 A full-stack lightweight LLM tutorial series: tokenizer, architecture, MoE, pretraining, LoRA, DPO, distillation and beyond.

本项目是一个轻量级但覆盖面广的 LLM 教程系列，适合希望**从底层理解大模型**的开发者、研究者与爱好者。我们从 tokenizer 开始，一步步搭建属于你自己的 LLM 模型，并逐步掌握主流的训练、优化与微调技巧。

---

## 📚 教程目录

### 基础构建
1. [如何从头训练 tokenizer](src/1-如何从头训练tokenizer.md)  
2. [一行代码之差，模型性能提升背后的 RMSNorm 玄机](src/2-一行代码之差，模型性能提升背后的RMSNorm玄机.md)  
3. [原始 Transformer 的位置编码及其缺陷](src/3-原始Transformer的位置编码及其缺陷.md)  
4. [旋转位置编码原理与应用全解析](src/4-旋转位置编码原理与应用全解析.md)  

### 架构进阶
5. [魔改的注意力机制：效率优化大盘点](src/5-魔改的注意力机制，细数当代LLM的效率优化手段.md)  
6. [从稠密到稀疏：详解专家混合模型 MoE](src/6-从稠密到稀疏，详解专家混合模型MOE.md)  
7. [像搭积木一样构建一个大模型](src/7-像搭积木一样构建一个大模型.md)  

### 模型训练与调优
8. [LLM 预训练流程全解](src/8-LLM预训练流程全解.md)  
9. [指令微调详解：让大模型从“能说”变得“会听”](src/9-指令微调详解-让大模型从“能说”变得“会听”.md)  
10. [DPO：大模型对齐训练的新范式](src/10-DPO-大模型对齐训练的新范式.md)  

### 模型优化与压缩
11. [LoRA：LLM 轻量化微调的利器](src/11-LoRA-LLM轻量化微调的利器.md)  
12. [从白盒到黑盒：全面掌握大模型蒸馏技术](src/12-从白盒到黑盒，全面掌握大模型蒸馏技术.md)  

---

## 🚀 快速开始

1. 克隆仓库：
```bash
git clone https://github.com/hans0809/MiniMind-in-Depth.git
cd MiniMind-in-Depth
```
